## Definition
$\hspace{0pt}2$ [[Random Variables|random variables]] $X$ and $Y$  in the same probability space $(\Omega,\mathfrak{F},\mathbb{P})$ are [[Independence|independent]] iff:
$$
\mathbb{P}(X \in C,Y\in D)=\mathbb{P}(X \in C)\mathbb{P}(Y\in D)
$$
For all $C,D$ such that $C\subseteq \chi,D\subseteq\Gamma$, known as the product of the marginals, or marginal split
Using the definition of random variables in terms of their [[Cumulative Distribution Functions|CDFs]], we write this as:
$$
\forall x,y\in \mathbb{R},~\mathbb{P}(X\leq x,Y\leq y)=\mathbb{P}(X\leq x)\mathbb{P}(Y\leq y)
$$
This can be rewritten for [[Discrete Random Variables|discrete variables]] as:
$$
p_{(X,Y)}(x,y)=p_{X}(x)p_{Y}(y)
$$
And [[Continuous Random Variables|continuous variables]] as:
$$
f(x,y)=f_{X}(x)f_{Y}(y)
$$
### Remark
One can show that the Borel [[Sigma Algebra|sigma algebra]] $\mathcal{B}(\mathbb{R})$ can be generated by the collection $\left\{ (-\infty,a]\mid a\in\mathbb{R} \right\}$. Consequently, the condition 
## Mutually Independent Random Variables
Suppose that $X_{1},X_{2},\dots,X_{k}$ are random variables, they are mutually independent if any of the following hold:
$$
\mathbb{P}(X_{1}\leq x_{1},\dots,X_{k}\leq x_{k})=\prod_{i=1}^{k}\mathbb{P}(X_{i}\leq x_{i}),\,\forall x_{1},\dots,x_{k}\in \mathbb{R}
$$
$$
\mathbb{P}(X_{1}\in  A_{1},\dots,X_{k}\in A_{k})=\prod_{i=1}^{k} \mathbb{P}(X_{i}\in A_{i}),\,\forall \text{ open sets }A_{1},\dots,A_{k}\subseteq \mathbb{R}
$$
$$
 \mathbb{E}\left( \prod_{i=1}^{k}f_{i}(X_{i}) \right)=\prod_{i=1}^{k}\mathbb{E}(f_{i}(x_{i}))
 
$$
### For Functions of Random Variables
If $X_{1}$ and $X_{2}$ are independent random variables, while functions $f_{1}:\mathbb{R}\to \mathbb{R}$ and $f_{2}:\mathbb{R}\to \mathbb{R}$ satisfy
$$
\forall y\in \mathbb{R},~f^{-1}((-\infty,y])\in \mathcal{B}(\mathbb{R})
$$
Where $\mathcal{B}(\mathbb{R})$ is the Borel [[Sigma Algebra|sigma algebra]] in $\mathbb{R}$, then $Y_{i}(\omega):=f_{i}(X_{i}(\omega))$ are also independent random variables, this extends to any finite or infinite setting
## [[Expectation|Expectation]]
If $X$ and $Y$ are independent, then $\mathbb{E}(XY)=\mathbb{E}(X)\mathbb{E}(Y)$ and more generally, 
$$
\mathbb{E}(g(X)h(Y))=\mathbb{E}(g(X))\mathbb{E}(h(Y))
$$
### Proof
Let us assume that $(X,Y)$ is jointly discrete with pmf $p(x,y)$
$$
\mathbb{E}(g(X)h(Y))=\sum_{x}\sum_{y}g(x)h(y)p(x,y)
$$
Since $X$ and $Y$ are independent, $p(x,y)=p_{X}(x)p_{Y}(y)$, which can be put into our sum as:
$$
=\sum_{x}\sum_{y}g(x)h(y)p_{X}(x)p_{Y}(y)=\left( \sum_{x}g(x)p_{X}(x) \right)\left( \sum_{y}h(y)p_{Y}(y) \right)
$$
$$
=\mathbb{E}(g(X))\mathbb{E}(h(Y))
$$
## Example
Fix arbitrary $A\in\mathcal{B}[0,1]$ and consider the corresponding [[Indicator Function|indicator random variable]] $\mathbb{1}_{A}$. The latter is [[Bernoulli Distribution|bernoulli]] distributed with parameter $\mathbb{E}(\mathbb{1}_{A})=\mathbb{P}(\mathbb{1}_{A}=1)=\mathbb{P}(A)$, the Lebesgue [[Measures|measure]] of $A$
Fix arbitrary $A,B\in\mathcal{B}[0,1]$, then
$$
\mathbb{1}_{A\cap B}(\omega)\equiv\mathbb{1}_{A}(\omega)\mathbb{1}_{B}(\omega)
$$
$$
 \mathbb{1}_{A^{c}}(\omega)\equiv1-\mathbb{1}_{A}(\omega)
$$
$$
 \mathbb{1}_{A\cup B}(\omega)\equiv \mathbb{1}_{A}(\omega)+\mathbb{1}_{B}(\omega)-\mathbb{1}_{A\cap B}(\omega)
$$
And so all set operations can be recorded as [[Linear Combinations|linear combinations]] of products of indicator functions. Furthermore, it is straightforward to check that:
$$
\text{events }A\text{ and }B\text{ are independent} \iff \text{random variables }\mathbb{1}_{A}\text{ and }\mathbb{1}_{B}\text{ are independent}
$$
Indeed, by the above, for all $C,D\in\mathcal{B}[0,1]$,
$$
\mathbb{P}(\mathbb{1}_{C}=1,\mathbb{1}_{D}=1)=\mathbb{P}(\mathbb{1}_{C}\mathbb{1}_{D}=1)=\mathbb{P}(\mathbb{1}_{C\cap D}=1)=\mathbb{P}(C\cap D)
$$
$$
 \mathbb{P}(C)\mathbb{P}(D)=\mathbb{P}(\mathbb{1}_{C}=1)\mathbb{P}(\mathbb{1}_{D}=1)
$$
Which are both expressions coinciding in the case that $C$ and $D$ are independent. Applying these relations to all combinations $C\in\left\{ A,A^{c} \right\}$ and $D\in\left\{ B,B^{c} \right\}$, one deduces that events are independent iff the random variables are
## Definition
Let $(\Omega,\mathfrak{F},\mathbb{P})$ be a [[Probability Spaces|probability space]], let $A,B\in \mathfrak{F}$ be $\hspace{0pt}2$ [[\mathbb{E}vents|events]], $A$ and $B$ are said to be independent if:
$$
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)
$$
Using the formula for [[Conditional Probability|conditional probability]]:
$$
\mathbb{P}(A\cap B)=\mathbb{P}(A|B)\mathbb{P}(B)
$$
So if two events are independent, then:
$$
\mathbb{P}(A|B)=\mathbb{P}(A)
$$
In other words, learning about $B$ will not tell us anything new about $A$, and similarly, learinning about $A$ will not tell us anything new about $B$
Note that independence cannot be observed on a Venn Diagram
___
If $A,B\in\mathfrak{F}$ are two events, where $B$ has vanishing probability, then $A$ and $B$ are independent:
$$
\mathbb{P}(B)=0\implies \mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)=0
$$
Similarly, if $B$ has probability $1$, then $A$ and $B$ are independent:
$$
\mathbb{P}(B)=1\implies \mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)=\mathbb{P}(A)
$$
## Multiple Events
A (possibily infinite) collection of events $\mathcal{A}\subseteq \mathfrak{F}$ are mutually independent if for every finite non-empty $\mathcal{C}\subseteq \mathcal{A}$, that is, $\mathcal{B}$ is a finite subcollection of the events in question,
$$
\mathbb{P}\left( \bigcap_{A\in \mathcal{C}}A \right)=\prod_{A\in \mathcal{C}}\mathbb{P}(A)
$$

## Complement
Show that if $A$ and $B$ are independent, then $A$ and $B^{c}$ are also independent
### Proof
We know that $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$, we have to also show that $\mathbb{P}(A\cap B^{c})=\mathbb{P}(A)\mathbb{P}(B^{c})$
Observe that $A=(A\cap B)\cup (A\cap B^{c})$, so:
$$
\mathbb{P}(A)=\mathbb{P}(A\cap B)+\mathbb{P}(A\cap B^{c})
$$
Since $(A\cap B)\cap(A\cap B^{c})=\emptyset$
So
$$
\mathbb{P}(A)=\mathbb{P}(A)\mathbb{P}(B)+\mathbb{P}(A\cap B^{c})
$$
$$
\implies \mathbb{P}(A\cap B^{c})
$$
## Example
Roll a fair die:
$$
A_{1}=\{ 2,4,6 \},A_{2}=\{ 3,6 \},A_{3}=\{ 4,5,6\}
$$
$$
\mathbb{P}(A_{1}\cap A_{2})=\mathbb{P}(\{ 6 \})=\frac{1}{6}
$$
$$
\mathbb{P}(A_{1})\mathbb{P}(A_{2})=\frac{3}{6}\times \frac{2}{6}=\frac{1}{6}=\mathbb{P}(A_{1}\cap A_{2})
$$
So $A_{1}$ and $A_{2}$ are independent


#Mathematics #Probability #Definition